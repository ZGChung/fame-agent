#!/usr/bin/env python3
"""
è§†é¢‘ç”Ÿæˆæ¨¡å—
æ”¯æŒå¤šç§ AI è§†é¢‘ç”ŸæˆæœåŠ¡

ä½¿ç”¨æ–¹å¼:
1. é…ç½®è§†é¢‘ç”ŸæˆæœåŠ¡ API
2. è°ƒç”¨ generate_video() ç”Ÿæˆè§†é¢‘
3. é›†æˆåˆ° pipeline

æ”¯æŒçš„è§†é¢‘ç”Ÿæˆæ–¹å¼:
- Runway ML (text to video, image to video)
- Pika Labs (text to video, image to video)
- Luma Dream Machine (image to video)
- æœ¬åœ° ffmpeg (å›¾ç‰‡ç”Ÿæˆè§†é¢‘)
"""

import os
import json
import asyncio
import subprocess
from pathlib import Path
from typing import Optional, List
from datetime import datetime

# é…ç½®
CONFIG_FILE = Path(__file__).parent.parent / "config.json"

def load_config() -> dict:
    """åŠ è½½é…ç½®"""
    if CONFIG_FILE.exists():
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


class VideoGenerator:
    """è§†é¢‘ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.config = load_config()
        self.video_config = self.config.get('video', {})
    
    def is_configured(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦é…ç½®äº†ä»»æ„è§†é¢‘æœåŠ¡"""
        return bool(
            self.video_config.get('runway_api_key') or
            self.video_config.get('pika_api_key') or
            self.video_config.get('luma_api_key') or
            self.video_config.get('heygen_api_key')
        )
    
    async def generate_from_images(
        self,
        images: List[str],
        output_path: str = None,
        duration_per_image: float = 3.0,
        transition: str = "fade"
    ) -> str:
        """
        ä»å›¾ç‰‡ç”Ÿæˆè§†é¢‘ï¼ˆå¹»ç¯ç‰‡ï¼‰
        
        Args:
            images: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            duration_per_image: æ¯å¼ å›¾ç‰‡æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            transition: è½¬åœºæ•ˆæœ (fade, slide, none)
        
        Returns:
            str: ç”Ÿæˆè§†é¢‘çš„è·¯å¾„
        """
        if not images:
            raise ValueError("éœ€è¦æä¾›è‡³å°‘ä¸€å¼ å›¾ç‰‡")
        
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"output/{timestamp}_video.mp4"
        
        # ä½¿ç”¨ ffmpeg ç”Ÿæˆè§†é¢‘
        success = await self._ffmpeg_slideshow(
            images, output_path, duration_per_image, transition
        )
        
        if success:
            return output_path
        raise RuntimeError("è§†é¢‘ç”Ÿæˆå¤±è´¥")
    
    async def _ffmpeg_slideshow(
        self,
        images: List[str],
        output_path: str,
        duration: float,
        transition: str
    ) -> bool:
        """ä½¿ç”¨ ffmpeg ç”Ÿæˆå¹»ç¯ç‰‡è§†é¢‘"""
        
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # å¦‚æœåªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œä½¿ç”¨ Ken Burns æ•ˆæœ
        if len(images) == 1:
            # Ken Burns: slow zoom in
            cmd = [
                'ffmpeg', '-y',
                '-loop', '1',
                '-i', images[0],
                '-c:v', 'libx264',
                '-t', str(duration),
                '-pix_fmt', 'yuv420p',
                '-vf', f'scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2,zoompan=z=\'min(zoom+0.001,1.5)\':d={int(duration*25)}:s=1080x1920',
                output_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            return result.returncode == 0
        
        # å¤šå¼ å›¾ç‰‡ï¼šåˆ›å»ºç‰‡æ®µååˆå¹¶
        try:
            temp_videos = []
            # ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿æ­£ç¡®
            output_dir = Path(output_path).resolve().parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            for i, img in enumerate(images):
                img_path = Path(img).resolve()
                temp_out = output_dir / f"temp_{i}.mp4"
                cmd = [
                    'ffmpeg', '-y',
                    '-loop', '1',
                    '-i', str(img_path),
                    '-c:v', 'libx264',
                    '-t', str(duration),
                    '-pix_fmt', 'yuv420p',
                    '-vf', f'scale=1080:1920:force_original_aspect_ratio=decrease,pad=1080:1920:(ow-iw)/2:(oh-ih)/2',
                    '-r', '30',
                    str(temp_out)
                ]
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
                if result.returncode != 0:
                    print(f"åˆ›å»ºç‰‡æ®µå¤±è´¥: {result.stderr[:200]}")
                    return False
                temp_videos.append(str(temp_out))
            
            # åˆå¹¶ - ä½¿ç”¨ concat demuxer æ–¹å¼
            concat_file = output_dir / "concat.txt"
            with open(concat_file, 'w') as f:
                for v in temp_videos:
                    f.write(f"file '{v}'\n")
            
            cmd = [
                'ffmpeg', '-y',
                '-f', 'concat',
                '-safe', '0',
                '-i', str(concat_file),
                '-c:v', 'libx264',
                '-preset', 'fast',
                '-crf', '23',
                '-c:a', 'aac',
                '-b:a', '128k',
                output_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            # æ¸…ç†
            for v in temp_videos:
                Path(v).unlink()
            concat_file.unlink()
            
            return result.returncode == 0
            
        except Exception as e:
            print(f"FFmpeg ç”Ÿæˆå¤±è´¥: {e}")
            return False
    
    async def generate_text_to_video(
        self,
        prompt: str,
        service: str = "runway",
        output_path: str = None
    ) -> str:
        """
        æ–‡å­—ç”Ÿæˆè§†é¢‘
        
        Args:
            prompt: è§†é¢‘æè¿°
            service: æœåŠ¡å•† (runway, pika, luma, seedance)
            output_path: è¾“å‡ºè·¯å¾„
        
        Returns:
            str: è§†é¢‘è·¯å¾„
        """
        if service == "runway":
            return await self._runway_generate(prompt, output_path)
        elif service == "pika":
            return await self._pika_generate(prompt, output_path)
        elif service == "luma":
            return await self._luma_generate(prompt, output_path)
        elif service == "seedance":
            return await self._seedance_generate(prompt, output_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æœåŠ¡: {service}")
    
    async def _runway_generate(self, prompt: str, output_path: str) -> str:
        """Runway ML API"""
        api_key = self.video_config.get('runway_api_key')
        if not api_key:
            raise ValueError("éœ€è¦é…ç½® Runway API Key")
        
        # TODO: å®ç° Runway API è°ƒç”¨
        # API: https://api.runwayml.com/v1/generation
        raise NotImplementedError("Runway API é›†æˆå¼€å‘ä¸­")
    
    async def _pika_generate(self, prompt: str, output_path: str) -> str:
        """Pika Labs API"""
        api_key = self.video_config.get('pika_api_key')
        if not api_key:
            raise ValueError("éœ€è¦é…ç½® Pika API Key")
        
        # TODO: å®ç° Pika API è°ƒç”¨
        raise NotImplementedError("Pika API é›†æˆå¼€å‘ä¸­")
    
    async def _luma_generate(self, prompt: str, output_path: str) -> str:
        """Luma Dream Machine API"""
        api_key = self.video_config.get('luma_api_key')
        if not api_key:
            raise ValueError("éœ€è¦é…ç½® Luma API Key")
        
        # TODO: å®ç° Luma API è°ƒç”¨
        raise NotImplementedError("Luma API é›†æˆå¼€å‘ä¸­")
    
    async def _seedance_generate(self, prompt: str, output_path: str) -> str:
        """
        Seedance 2.0 API
        
        æ³¨æ„: Seedance 2.0 å¯èƒ½å°šæœªå…¬å¼€å‘å¸ƒï¼Œç­‰å¾…å®˜æ–¹ API
        """
        api_key = self.video_config.get('seedance_api_key')
        if not api_key:
            # å¦‚æœæ²¡æœ‰ API Keyï¼Œä½¿ç”¨æœ¬åœ°ç”Ÿæˆä½œä¸ºå¤‡é€‰
            print("âš ï¸ Seedance API Key æœªé…ç½®ï¼Œä½¿ç”¨æœ¬åœ°è§†é¢‘ç”Ÿæˆ")
            # è¿”å›ç©ºå­—ç¬¦ä¸²è®©è°ƒç”¨æ–¹ä½¿ç”¨æœ¬åœ°ç”Ÿæˆ
            return ""
        
        # TODO: Seedance 2.0 API é›†æˆ
        # ç­‰å¾…å®˜æ–¹å‘å¸ƒåå®ç°
        raise NotImplementedError("Seedance 2.0 API é›†æˆå¼€å‘ä¸­ - ç­‰å¾…å®˜æ–¹å‘å¸ƒ")
    
    async def add_audio(
        self,
        video_path: str,
        audio_path: str = None,
        tts_text: str = None,
        output_path: str = None
    ) -> str:
        """
        ä¸ºè§†é¢‘æ·»åŠ éŸ³é¢‘
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (å¯é€‰)
            tts_text: è¦è½¬è¯­éŸ³çš„æ–‡å­— (å¯é€‰)
            output_path: è¾“å‡ºè·¯å¾„
        
        Returns:
            str: å¸¦éŸ³é¢‘çš„è§†é¢‘è·¯å¾„
        """
        if not output_path:
            output_path = video_path.replace('.mp4', '_with_audio.mp4')
        
        # å¦‚æœæä¾›äº† ttsï¼Œä½¿ç”¨ TTS ç”ŸæˆéŸ³é¢‘
        if tts_text and not audio_path:
            audio_path = await self._generate_tts(tts_text)
            if not audio_path:
                raise ValueError("TTS ç”Ÿæˆå¤±è´¥")
        
        if not audio_path:
            raise ValueError("éœ€è¦æä¾›éŸ³é¢‘æ–‡ä»¶æˆ– TTS æ–‡å­—")
        
        # åˆå¹¶è§†é¢‘å’ŒéŸ³é¢‘
        cmd = [
            'ffmpeg', '-y',
            '-i', video_path,
            '-i', audio_path,
            '-c:v', 'copy',
            '-c:a', 'aac',
            '-shortest',
            output_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            return output_path
        raise RuntimeError(f"éŸ³é¢‘åˆå¹¶å¤±è´¥: {result.stderr}")
    
    async def _generate_tts(self, text: str, output_path: str = None, voice: str = None) -> str:
        """ç”Ÿæˆ TTS éŸ³é¢‘
        
        ä¼˜å…ˆä½¿ç”¨ ElevenLabsï¼ˆå¦‚æœé…ç½®ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ macOS say å‘½ä»¤
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡å­—
            output_path: è¾“å‡ºè·¯å¾„
            voice: è¯­éŸ³åç§° (é»˜è®¤è‡ªåŠ¨é€‰æ‹©ä¸­/è‹±æ–‡)
        """
        # ä¼˜å…ˆä½¿ç”¨ ElevenLabs
        tts_config = self.video_config.get('tts', {})
        elevenlabs_key = tts_config.get('api_key') or os.getenv('ELEVENLABS_API_KEY')
        
        if elevenlabs_key:
            return await self._generate_elevenlabs(text, output_path, voice, elevenlabs_key)
        
        # å›é€€åˆ° macOS say å‘½ä»¤
        return await self._generate_tts_say(text, output_path, voice)
    
    async def _generate_elevenlabs(
        self, 
        text: str, 
        output_path: str = None, 
        voice: str = None,
        api_key: str = None
    ) -> str:
        """ä½¿ç”¨ ElevenLabs ç”Ÿæˆ TTS"""
        import urllib.request
        import urllib.parse
        
        tts_config = self.video_config.get('tts', {})
        voice_id = voice or tts_config.get('voice_id', 'rachel')
        
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"output/{timestamp}_tts.mp3"
        
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ElevenLabs API
        url = f"https://api.elevenlabs.io/v1/text_to_speech/{voice_id}"
        
        headers = {
            'Accept': 'audio/mpeg',
            'Content-Type': 'application/json',
            'xi-api-key': api_key
        }
        
        data = json.dumps({
            'text': text,
            'model_id': 'eleven_monolingual_v1',
            'voice_settings': {
                'stability': 0.5,
                'similarity_boost': 0.75
            }
        }).encode('utf-8')
        
        req = urllib.request.Request(url, data=data, headers=headers, method='POST')
        
        try:
            with urllib.request.urlopen(req, timeout=30) as response:
                with open(output_path, 'wb') as f:
                    f.write(response.read())
            return output_path
        except Exception as e:
            raise RuntimeError(f"ElevenLabs TTS å¤±è´¥: {e}")
    
    async def _generate_tts_say(self, text: str, output_path: str = None, voice: str = None) -> str:
        """ä½¿ç”¨ macOS say å‘½ä»¤ç”Ÿæˆ TTS
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡å­—
            output_path: è¾“å‡ºè·¯å¾„
            voice: è¯­éŸ³åç§° (é»˜è®¤è‡ªåŠ¨é€‰æ‹©ä¸­/è‹±æ–‡)
        """
        # è‡ªåŠ¨é€‰æ‹©è¯­éŸ³
        if not voice:
            # ç®€å•çš„ä¸­è‹±æ–‡æ£€æµ‹
            if any('\u4e00' <= c <= '\u9fff' for c in text):
                voice = "Tingting"  # ä¸­æ–‡å¥³å£°
            else:
                voice = "Daniel"  # è‹±æ–‡å­—éŸ³
        
        # å…ˆç”Ÿæˆ aiff
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        aiff_path = f"output/{timestamp}_tts.aiff"
        
        # è¾“å‡ºè·¯å¾„
        if output_path and output_path.endswith('.mp3'):
            mp3_path = output_path
        else:
            mp3_path = f"output/{timestamp}_tts.mp3"
        
        output_dir = Path(aiff_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨ say å‘½ä»¤ç”ŸæˆéŸ³é¢‘ (ä½¿ç”¨ -v æŒ‡å®šè¯­éŸ³)
        cmd = ['say', '-v', voice, text, '-o', aiff_path]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode != 0:
            # å¤‡ç”¨ï¼šå°è¯•é»˜è®¤è¯­éŸ³
            cmd = ['say', text, '-o', aiff_path]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode != 0:
            raise RuntimeError(f"TTS ç”Ÿæˆå¤±è´¥: {result.stderr}")
        
        # è½¬æ¢æ ¼å¼ä¸º MP3
        cmd = [
            'ffmpeg', '-y',
            '-i', aiff_path,
            '-acodec', 'libmp3lame',
            '-ar', '22050',
            mp3_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # åˆ é™¤ aiff
        Path(aiff_path).unlink()
        
        if result.returncode == 0:
            return mp3_path
        
        raise RuntimeError(f"MP3 è½¬æ¢å¤±è´¥: {result.stderr}")


# ä¾¿æ·å‡½æ•°
async def quick_video(
    images: List[str],
    output_path: str = None,
    with_tts: str = None
) -> str:
    """å¿«é€Ÿç”Ÿæˆè§†é¢‘"""
    generator = VideoGenerator()
    
    # ç”Ÿæˆè§†é¢‘
    video_path = await generator.generate_from_images(images, output_path)
    
    # æ·»åŠ éŸ³é¢‘
    if with_tts:
        video_path = await generator.add_audio(video_path, tts_text=with_tts)
    
    return video_path


if __name__ == "__main__":
    import sys
    
    gen = VideoGenerator()
    print("ğŸ¬ è§†é¢‘ç”Ÿæˆå™¨")
    print(f"é…ç½®çŠ¶æ€: {'âœ… å·²é…ç½®' if gen.is_configured() else 'âŒ æœªé…ç½®'}")
    
    if len(sys.argv) > 1:
        # æµ‹è¯•ç”Ÿæˆ
        images = sys.argv[1:]
        print(f"ä» {len(images)} å¼ å›¾ç‰‡ç”Ÿæˆè§†é¢‘...")
        
        async def test():
            path = await gen.generate_from_images(images)
            print(f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸ: {path}")
        
        asyncio.run(test())
